{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGuONxZkb6NV",
        "outputId": "b0e0399f-8fa2-4c20-b925-2dad56558f36"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!touch HelloCUDA.cu"
      ],
      "metadata": {
        "id": "tW5P4ZNp0ml3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vIz_DIELOlqA"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_75 ./HelloCUDA.cu -o HelloCUDA.out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./HelloCUDA.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXsJ_cD-a5a8",
        "outputId": "2b0253ee-8a77-4d85-8f5f-13200d26046c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World From CPU!\n",
            "Hello World From GPU!\n",
            "Hello World From GPU!\n",
            "Hello World From GPU!\n",
            "Hello World From GPU!\n",
            "Hello World From GPU!\n",
            "Hello World From GPU!\n",
            "Hello World From GPU!\n",
            "Hello World From GPU!\n",
            "Hello World From GPU!\n",
            "Hello World From GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN5LyZ3gg489",
        "outputId": "8ae74edf-0aa2-4397-88e7-2c54718b38f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycuda\n",
            "  Downloading pycuda-2025.1.tar.gz (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2025.1.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from pycuda) (4.3.7)\n",
            "Requirement already satisfied: mako in /usr/lib/python3/dist-packages (from pycuda) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from pytools>=2011.2->pycuda) (4.13.2)\n",
            "Downloading pytools-2025.1.2-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2025.1-cp311-cp311-linux_x86_64.whl size=660424 sha256=bc2941d23a6222f377ad725f8b4388a36201dac2b5f90cac0da9d7bcbdf6af63\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/7e/6c/d2d1451ea6424cdc3d67b36c16fa7111eafdf2034bc3405666\n",
            "Successfully built pycuda\n",
            "Installing collected packages: pytools, pycuda\n",
            "Successfully installed pycuda-2025.1 pytools-2025.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pycuda.autoinit\n",
        "import pycuda.driver as cuda\n",
        "from pycuda.compiler import SourceModule"
      ],
      "metadata": {
        "id": "LPuU6NXMhqgO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pycuda.autoinit\n",
        "import pycuda.driver as cuda\n",
        "from pycuda.compiler import SourceModule\n",
        "import numpy\n",
        "\n",
        "a = numpy.random.rand(4,4)\n",
        "a = a.astype(numpy.float32)\n",
        "a_gpu = cuda.mem_alloc(a.nbytes)\n",
        "\n",
        "cuda.memcpy_htod(a_gpu, a)\n",
        "\n",
        "mod = SourceModule(\"\"\"\n",
        "__global__ void doublify(float *a)\n",
        "{\n",
        "    int idx = threadIdx.x + threadIdx.y*4;\n",
        "    a[idx] *= 2;\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "func = mod.get_function(\"doublify\")\n",
        "func(a_gpu, block=(4,4,1))\n",
        "\n",
        "a_doubled = numpy.empty_like(a)\n",
        "cuda.memcpy_dtoh(a_doubled, a_gpu)\n",
        "print(a_doubled)\n",
        "print()\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zgyJNPh1VI2",
        "outputId": "b90eb651-4ba2-4c0f-c01e-e32610522836"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/colab/_variable_inspector.py:27: UserWarning: device_allocation in out-of-thread context could not be cleaned up\n",
            "  globals().clear()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.15587677 1.9530303  0.07384459 0.25495884]\n",
            " [1.2085888  1.9317049  0.32392386 0.2994609 ]\n",
            " [1.172012   1.7717297  0.32822943 0.06193965]\n",
            " [1.3699086  1.6621329  0.17902164 1.2539927 ]]\n",
            "\n",
            "[[0.07793839 0.9765152  0.03692229 0.12747942]\n",
            " [0.6042944  0.96585244 0.16196193 0.14973044]\n",
            " [0.586006   0.88586485 0.16411471 0.03096983]\n",
            " [0.6849543  0.8310664  0.08951082 0.62699634]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pycuda.gpuarray as gpuarray\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "import numpy\n",
        "\n",
        "a_gpu = gpuarray.to_gpu(numpy.random.randn(4,4).astype(numpy.float32))\n",
        "a_doubled = (2*a_gpu).get()\n",
        "print(a_doubled)\n",
        "print(a_gpu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpDrzdDz1fcW",
        "outputId": "803bd119-d305-41cb-971d-871d2c45dbc6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/colab/_variable_inspector.py:27: UserWarning: device_allocation in out-of-thread context could not be cleaned up\n",
            "  globals().clear()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2.0264268   2.180804   -0.5207059  -0.03173042]\n",
            " [ 1.0470687  -2.9055848  -3.2619438  -1.3234549 ]\n",
            " [ 0.1899652   0.39841473 -1.0177335  -0.07355231]\n",
            " [ 0.96175086 -4.2567806   0.41256478  0.8278906 ]]\n",
            "[[ 1.0132134   1.090402   -0.26035294 -0.01586521]\n",
            " [ 0.52353436 -1.4527924  -1.6309719  -0.6617274 ]\n",
            " [ 0.0949826   0.19920737 -0.5088667  -0.03677616]\n",
            " [ 0.48087543 -2.1283903   0.20628239  0.4139453 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pycuda.driver as drv\n",
        "import pycuda.tools\n",
        "import pycuda.autoinit\n",
        "import numpy\n",
        "import numpy.linalg as la\n",
        "from pycuda.compiler import SourceModule\n",
        "\n",
        "mod = SourceModule(\"\"\"\n",
        "__global__ void multiply_them(float *dest, float *a, float *b)\n",
        "{\n",
        "  const int i = threadIdx.x;\n",
        "  dest[i] = a[i] * b[i];\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "multiply_them = mod.get_function(\"multiply_them\")\n",
        "\n",
        "a = numpy.random.randn(400).astype(numpy.float32)\n",
        "b = numpy.random.randn(400).astype(numpy.float32)\n",
        "\n",
        "dest = numpy.zeros_like(a)\n",
        "multiply_them(\n",
        "        drv.Out(dest), drv.In(a), drv.In(b),\n",
        "        block=(400,1,1))\n",
        "\n",
        "print(dest-a*b)"
      ],
      "metadata": {
        "id": "_Qa33Z7u2tjW",
        "outputId": "f78aaa15-cc28-41e1-c14a-169461b8c70f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    }
  ]
}